---
title: "Final Project"
subtitle: |
  | Final Project 
  | Data Science 1 with R (STAT 301-2)
author: "Miracle Ramos"
pagetitle: "Final Project Miracle Ramos"
date: today

format:
  html:
    toc: true
    toc-depth: 4
    toc-location: left
    embed-resources: true
    link-external-newwindow: true

execute:
  warning: false

from: markdown+emoji
reference-location: margin
citation-location: margin
---



::: {.callout-tip icon=false}

## Github Repo Link

[Miracle's GitHub Repo Link (miracleramos2025)](https://github.com/stat301-2-2025-winter/final-project-2-miracleramos2025.git)

:::

## Introduction

#### Research Objective
The goal of this project is to develop a predictive model that forecasts Airbnb rental prices in New York City. Specifically, I will predict the price of an Airbnb listing based on various features such as location, property type, number of reviews, availability, and host details.

#### Prediction Type
This is a regression problem, where the target variable will be the `price` of an Airbnb listing. The model will be trained using supervised learning techniques.

#### Motivation
As Airbnb continues to be a dominant platform in the short-term rental market, understanding pricing trends is crucial for both hosts and guests. This project allows me to explore machine learning techniques while analyzing factors that contribute to Airbnb pricing in NYC. By predicting prices accurately, I aim to provide insights into market trends and pricing strategies.

## Data Overview

#### Primary Dataset
The dataset used for this project is sourced from [Kaggle: New York City Airbnb Open Data](https://www.kaggle.com/datasets/dgomonov/new-york-city-airbnb-open-data?select=AB_NYC_2019.csv). It contains details about **Airbnb listings in NYC from 2019**, including **host details, property characteristics, location, reviews, and pricing.**

#### Datasets Used

  1. `AB_NYC_2019.csv` â€“ Includes host information, listing details, location, and pricing metrics.

**Description:** 
This dataset provides a comprehensive view of Airbnb activity in NYC, making it suitable for building a model that predicts listing prices based on various factors.

**Citations:** <br>

  - Kaggle: Dgomonov, [New York City Airbnb Open Data](https://www.kaggle.com/datasets/dgomonov/new-york-city-airbnb-open-data?select=AB_NYC_2019.csv)


#### Data Quality Check

```{r}
#| label: tbl-data-quality-check
#| tbl-cap: "Data Quality Check"
#| echo: false

# load packages
library(knitr)

# load saved data summary table
load("data/dataset_summary.rda")
load("data/var_type_summary.rda")

# display
kable(dataset_summary, caption = "Data Quality Summary")
kable(var_type_summary, caption = "Variable Type Summary")

```


#### Variable Types

  - **Categorical Variables:** There are 5 categorical variables, `name`, `host_name`, `neighborhood_group`, `neighborhood`, and `room_type` in the New York City Airbnb Open Data dataset.
  - **Date Variables:** `last_review` is the only date variable in the New York City Airbnb Open Data dataset.
  - **Numerical Variables:** There are 10 numerical variables, `id`, `host_id`, `latitude`, `longitude`, `price`, `minimum_nights`, `number_of_reviews`, `reviews_per_month`, `calculated_host_listings_count`, and `availability_365` in the New York City Airbnb Open Data dataset.

#### Missingness

  - The dataset has an overall missingness percentage of **2.57%**.
  - The `reviews_per_month` column has **10,052** missing values, which corresponds to listings with no reviews.
  - The `last_review` column also has **10,052** missing values, indicating that these listings have never received a review.
  - No missing values were found in critical numeric columns like `price`, `minimum_nights`, and `availability_365`.

#### Data Tidying 

  - Fill missing values in `reviews_per_month` with 0 where no reviews exist.
  - Convert `last_review` to a standardized date format.
  - Encode categorical variables like `neighbourhood_group` and `room_type` for modeling.


## Target variable analysis
#### Target Variable
The primary target variable for this analysis is `price`, a numerical variable representing the nightly cost of an Airbnb listing. Understanding price variations is important for both hosts and guests because it reflects market trends, demand, and listing characteristics. Predicting price based on key features such as location, property type, and availability will help identify factors that influence Airbnb rental pricing in New York City.

#### Univariate Analysis
```{r}
#| label: tbl-price-summary
#| tbl-cap: "Price Summary"
#| echo: false

# Load required packages
library(ggplot2)
library(dplyr)
library(knitr)
library(readr)

# Load dataset
airbnb_data <- read_csv("data/AB_NYC_2019.csv")

# Summarize price data
price_summary <- airbnb_data %>% summarize(
  mean_price = mean(price, na.rm = TRUE),
  median_price = median(price, na.rm = TRUE),
  max_price = max(price, na.rm = TRUE),
  min_price = min(price, na.rm = TRUE),
  std_dev = sd(price, na.rm = TRUE)
)

# Display summary table
kable(price_summary)
```
The **average Airbnb price in NYC is $152.72** but the median is $106, this shows that a few expensive listings raise the average. Prices range from $0 (possibly promotions or errors) to $10,000, with a high standard deviation of $240.15. This indicates large price differences across listings.
  
#### Symmetry & Skewness
```{r}
#| label: fig-price-dist
#| fig-cap: "Distribution of Airbnb Prices"
#| echo: false

# Plot price distribution
ggplot(airbnb_data, aes(x = price)) +
  geom_density(fill = "slateblue1", alpha = 0.5) +
  xlim(0, 1000) + 
  labs(title = "Density Plot of Airbnb Prices in NYC", x = "Price ($)", y = "Density")

```

The distribution is highly **right-skewed**, with a small number of extremely high priced listings. Most listings fall under $500 per night but outliers significantly increase the maximum price.




#### Missingness
```{r}
#| label: tbl-price-missing
#| tbl-cap: "Missing Values in Price"
#| echo: false

missing_price <- sum(is.na(airbnb_data$price))
kable(data.frame(Variable = "Price", Missing_Values = missing_price))
```

There are **no missing values** in the `price` variable, all listings have a recorded price. Since the target variable is complete, this will allow for more accurate modeling and analysis.


#### Transformations Considered

```{r}
#| label: fig-log-price-dist
#| fig-cap: "Log10 Transforation on Distribution of Prices"
#| echo: false

# apply log10 transformation
airbnb_data <- airbnb_data |>
  mutate(log_price = log10(price + 1))

# Plot transformed price distribution
ggplot(airbnb_data, aes(x = log_price)) +
  geom_density(fill = "skyblue1", alpha = 0.5) +
  labs(title = "Log10 Transformation Density of Airbnb Prices", x = "Log10(Price + 1)", y = "Density")

```

The **log10 transformation** of Airbnb prices **helps reduce skewness** and creates a more normalized distribution, making it easier for regression models to capture patterns in the data. The transformed distribution is more symmetric and indicates that extreme price outliers have less influence on the model.


## Methods

#### Data Splitting
  - **Training Set**: 80%
  - **Testing Set**: 20%
  - **Resampling Method**: 3-fold cross-validation with 3 repeats

#### Models Considered
To predict Airbnb listing prices in NYC, six models were implemented:

- **Baseline Model**: Predicts the mean price as a simple benchmark.
- **Linear Model**: Establishes a basic regression baseline.
- **Elastic Net Regression**: Balances lasso and ridge penalties for variable selection.
- **K-Nearest Neighbors (KNN)**: A distance-based model that predicts prices based on the similarity of nearby listings.
- **Random Forest**: Captures complex relationships with an ensemble of decision trees.
- **Boosted Trees**: Further refines tree-based predictions using boosting techniques.

#### Feature Engineering
To prepare the dataset for modeling, four structured preprocessing recipes were developed:

  - **Recipe 1 (Basic)** (`2_recipe1_basic.R`): Standardizes numerical variables and applies one-hot encoding for categorical variables.
  - **Recipe 1 (Advanced)** (`2_recipe1_advanced.R`): Builds on the basic version by adding feature interactions and polynomial terms while removing redundant predictors.
  - **Recipe 2 (Basic)** (`3_recipe2_basic.R`): Uses a different transformation approach suited for models that require less preprocessing.
  - **Recipe 2 (Advanced)** (`3_recipe2_advanced.R`): Incorporates additional transformations, such as non-linear feature engineering and refined categorical encoding.
  
#### Hyperparameter Tuning
Hyperparameters for selected models were optimized using grid search with 3-fold cross-validation and 3 repeats:

  - **Elastic Net Regression**: Tuned `penalty` (regularization strength) and `mixture` (balance between ridge and lasso).
  - **K-Nearest Neighbors (KNN)**: Tuned `neighbors (k)` to determine the optimal number of listings to reference.
  - **Random Forest**: Tuned `mtry` (number of predictors considered at each split) and `min_n` (minimum number of observations in a terminal node).
  - **Boosted Trees**: Tuned `mtry`, `min_n`, and `learn_rate`. A log10 transformation was applied to `learn_rate` to improve tuning efficiency.
  
#### Assessment Metric
Model performance will be evaluated using **Root Mean Squared Error (RMSE)** because it emphasizes large errors and aligns with the goal of predicting prices accurately.


## Model Performance & Selection Results

#### Model Performance Summary

Root Mean Squared Error (RMSE) was used to evaluate model performance and determine the best model for predicting Airbnb prices.

The table below summarizes the RMSE values for each model:
```{r}
#| label: tbl-model-performance-summary
#| tbl-cap: "Model Performance Summary"
#| echo: false

library(knitr)
library(here)
library(kableExtra)

# load model summary performance
load("results/model_performance_summary.rda")

# keep only relevant columns for this table
model_performance_summary <- model_performance %>%
  select(Model, RMSE)

# display results with better spacing
kable(model_performance_summary)

```
From @tbl-model-performance-summary, it is evident that the **Linear Model had the lowest RMSE (157.82), making it the best performing model**. The Random Forest model was the next best, while Elastic Net, KNN, and Boosted Trees performed worse than expected.


#### Tuning Parameter Analysis
The following table outlines the best hyperparameters selected for each tuned model:
```{r}
#| label: tbl-tuning-parameters
#| tbl-cap: "Tuned Model Parameters"
#| echo: false

library(dplyr)
library(knitr)

# filter for models that had hyperparameter tuning
tuned_models <- model_performance %>%
  filter(Model %in% c("Elastic Net", "KNN", "Random Forest", "Boosted Trees")) %>%
  select(Model, tuning_parameter) %>%  # Correct column reference
  rename(`Tuning Parameters` = tuning_parameter)  # Rename for display

# display results
kable(tuned_models)

```

#### Analysis of Tuning Results

  - **Elastic Net Regression**: The best parameters selected were mixture = 0.222 (closer to ridge regression) and penalty = 1. This suggests that a small amount of variable selection was needed, but regularization played a more significant role.
  - **KNN**: The best `neighbors` value was 44. Predictions were based on an average of the 44 closest listings.
  - **Random Forest**: The best combination was `mtry = 8` and `min_n = 4`. 8 predictors were considered at each split and terminal nodes required at least 4 observations.
  - **Boosted Trees**: The best parameters were `mtry = 2`, `min_n = 10`, and `learn_rate = 1.0023`. The learning rate being so high suggests the model may not have been effectively learning small improvements which lead to suboptimal performance.
  
#### Further Tuning Considerations
While further tuning could be explored, there were computational and memory limitations that affected the extent of hyperparameter optimization:

  - **Boosted Trees required significant memory and processing power**. Due to system constraints, testing a finer grid for the learning rate (0.001â€“0.1 instead of 1.0023) was not feasible.
  - **KNN performed moderately well**, but the choice of 44 neighbors may indicate that a smaller or larger value could perform better. Testing in increments of 5 within 30â€“50 might have been useful.
  - **Elastic Net leaned towards ridge regression**, variable selection was less critical. Future analysis could test pure ridge regression to confirm.

Given these challenges, future work could explore more efficient implementations. For example, optimized gradient boosting libraries that might yield better performance with lower memory requirements.

#### Final Model Selection
After analyzing the model performances, the **Linear Model** was chosen as the final model because:

  - It had the **lowest RMSE (157.82)**.
  - **It outperformed all tuned models** including complex models like Random Forest and Boosted Trees.
  - **It is interpretable**, insights can be drawn from coefficients about which factors influence Airbnb prices.
  - The additional **complexity of tree-based methods did not justify their marginal improvements**.
  
##### Was this result surprising?
Yes and no. Initially, it was expected that Random Forest or Boosted Trees might perform best since they capture nonlinear relationships. However, since Airbnb pricing is influenced by structured factors like location, room type, and availability, a Linear Modelâ€™s assumptions may actually align well with the dataset. The poor performance of Boosted Trees was surprising but given the high learning rate issue and computational constraints, this result makes sense.

## Final Model Analysis

#### Model Fit & Evaluation
```{r}
#| label: tbl-final-model-performance
#| tbl-cap: "Final Model Summary"
#| echo: false
#| eval: false

library(knitr)
library(here)

# load final model performance
load("results/final_model_performance.rda")

# display results
kable(final_rmse)

```


  

## Conclusion


#### Future Work


## References

  - Kaggle: Kaggle: *Dgomonov, New York City Airbnb Open Data*. Link

## Comment on Generative AI Use



